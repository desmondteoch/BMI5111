{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc832f8",
   "metadata": {},
   "source": [
    "### stat-RISMED program\n",
    "Objective: To build a classification model to identify whether overseas regulatory action has been taken for a substandard medicine issue, based on the case description.\n",
    "* Output of model will be used as input for stat-RISMED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7628875",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f23308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9596cbe-fd1e-4a02-9530-dfb45fe79d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from transformers import BertTokenizer, TFAutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, roc_auc_score, f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345fe2f0-3cd8-4ecf-8331-016a77628a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\desmondteoch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e21c69",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc6110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load full data\n",
    "os.chdir(r\"C:\\Users\\desmondteoch\\Documents\\CVU\\Capstone\\Impact Score\")\n",
    "data = pd.read_csv(\"Impact Score_2011tillJun2021_updated28Aug2021.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b2265f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 893 entries, 0 to 892\n",
      "Data columns (total 8 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   case_no                 893 non-null    object\n",
      " 1   date_of_receipt         893 non-null    object\n",
      " 2   created_time            893 non-null    object\n",
      " 3   case_title              893 non-null    object\n",
      " 4   case_description        893 non-null    object\n",
      " 5   rismed_defect           893 non-null    object\n",
      " 6   rismed_defect_severity  893 non-null    object\n",
      " 7   rismed_overseas_action  893 non-null    object\n",
      "dtypes: object(8)\n",
      "memory usage: 55.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# select required columns only\n",
    "overseas = data[[\"case_no\",\"date_of_receipt\",\"created_time\",\"case_title\",\"case_description\",\\\n",
    "                \"rismed_defect\",\"rismed_defect_severity\",\"rismed_overseas_action\"]]\n",
    "overseas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db6882c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      " label\n",
      "1    569\n",
      "0    324\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# recode \"rismed_overseas_action\" -- combine \"Yes\" and \"Unknown\" into one\n",
    "overseas.loc[:, \"label\"] = overseas[\"rismed_overseas_action\"].replace({\"Yes\": \"Yes\", \"Unknown\": \"Yes\",\\\n",
    "                                                                       \"No\": \"No\"})\n",
    "le = LabelEncoder()\n",
    "overseas.loc[:, \"label\"] = le.fit_transform(overseas[\"label\"])\n",
    "print(\"Class distribution:\\n\",overseas.label.value_counts())\n",
    "# 1: \"Yes\", 0: \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f0b1c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_no</th>\n",
       "      <th>date_of_receipt</th>\n",
       "      <th>created_time</th>\n",
       "      <th>case_title</th>\n",
       "      <th>case_description</th>\n",
       "      <th>rismed_defect</th>\n",
       "      <th>rismed_defect_severity</th>\n",
       "      <th>rismed_overseas_action</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVS2011-000102</td>\n",
       "      <td>25/2/2009</td>\n",
       "      <td>2/6/2011</td>\n",
       "      <td>Certain batches of Hospira's Desferrioxamine m...</td>\n",
       "      <td>Certain batches of Hospira's Desferrioxamine m...</td>\n",
       "      <td>Product physical issue</td>\n",
       "      <td>Low</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Certain batches of Hospira's Desferrioxamine m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVS2011-000354</td>\n",
       "      <td>10/3/2011</td>\n",
       "      <td>13/6/2011</td>\n",
       "      <td>NUH found some batches of Adrenaline 1:1000 in...</td>\n",
       "      <td>NUH found some batches of adrenaline 1:1000 in...</td>\n",
       "      <td>Product expiration date missing, illegible or ...</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>NUH found some batches of adrenaline 1:1000 in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVS2011-000368</td>\n",
       "      <td>19/4/2011</td>\n",
       "      <td>1/7/2011</td>\n",
       "      <td>Dimenate tablets failed specification for assay</td>\n",
       "      <td>Drug Houses of Australia (DHA) informed HSA th...</td>\n",
       "      <td>Out of specification or out of trend test result</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Drug Houses of Australia (DHA) informed HSA th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVS2011-000374</td>\n",
       "      <td>16/11/2010</td>\n",
       "      <td>4/7/2011</td>\n",
       "      <td>Recall of specific lots of Ebetrexat 100mg/ml ...</td>\n",
       "      <td>EB had received two separate alerts from Austr...</td>\n",
       "      <td>Contamination with glass and/or metal particle</td>\n",
       "      <td>High</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>EB had received two separate alerts from Austr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVS2011-000381</td>\n",
       "      <td>5/1/2011</td>\n",
       "      <td>4/7/2011</td>\n",
       "      <td>Hundreds in UK become pregnant despite contrac...</td>\n",
       "      <td>Nearly 600 women in UK who used a popular cont...</td>\n",
       "      <td>Lack of efficacy</td>\n",
       "      <td>Medium</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Nearly 600 women in UK who used a popular cont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          case_no date_of_receipt created_time  \\\n",
       "0  SVS2011-000102       25/2/2009     2/6/2011   \n",
       "1  SVS2011-000354       10/3/2011    13/6/2011   \n",
       "2  SVS2011-000368       19/4/2011     1/7/2011   \n",
       "3  SVS2011-000374      16/11/2010     4/7/2011   \n",
       "4  SVS2011-000381        5/1/2011     4/7/2011   \n",
       "\n",
       "                                          case_title  \\\n",
       "0  Certain batches of Hospira's Desferrioxamine m...   \n",
       "1  NUH found some batches of Adrenaline 1:1000 in...   \n",
       "2    Dimenate tablets failed specification for assay   \n",
       "3  Recall of specific lots of Ebetrexat 100mg/ml ...   \n",
       "4  Hundreds in UK become pregnant despite contrac...   \n",
       "\n",
       "                                    case_description  \\\n",
       "0  Certain batches of Hospira's Desferrioxamine m...   \n",
       "1  NUH found some batches of adrenaline 1:1000 in...   \n",
       "2  Drug Houses of Australia (DHA) informed HSA th...   \n",
       "3  EB had received two separate alerts from Austr...   \n",
       "4  Nearly 600 women in UK who used a popular cont...   \n",
       "\n",
       "                                       rismed_defect rismed_defect_severity  \\\n",
       "0                             Product physical issue                    Low   \n",
       "1  Product expiration date missing, illegible or ...                 Medium   \n",
       "2   Out of specification or out of trend test result                 Medium   \n",
       "3     Contamination with glass and/or metal particle                   High   \n",
       "4                                   Lack of efficacy                 Medium   \n",
       "\n",
       "  rismed_overseas_action label  \\\n",
       "0                    Yes     1   \n",
       "1                Unknown     1   \n",
       "2                    Yes     1   \n",
       "3                    Yes     1   \n",
       "4                     No     0   \n",
       "\n",
       "                                                text  \n",
       "0  Certain batches of Hospira's Desferrioxamine m...  \n",
       "1  NUH found some batches of adrenaline 1:1000 in...  \n",
       "2  Drug Houses of Australia (DHA) informed HSA th...  \n",
       "3  EB had received two separate alerts from Austr...  \n",
       "4  Nearly 600 women in UK who used a popular cont...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overseas.loc[:, \"text\"] = overseas[\"case_description\"]\n",
    "overseas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a70caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(context):\n",
    "    clean_lst = [\"\\r\",\"\\n\",\"   \",'\"',\"'s\",'\\d']\n",
    "    punctuation_signs = list(\".?:!,;/\\{[}]@#$%^&*(|)-ï¿½\")\n",
    "    for x in clean_lst:\n",
    "        context = context.str.replace(x,'')\n",
    "    for punct in punctuation_signs:\n",
    "        context = context.str.replace(punct,' ')\n",
    "    context = context.str.lower()\n",
    "    return context\n",
    "\n",
    "def stop_words(context):\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    for stop_word in stop_words:\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        context = context.str.replace(regex_stopword, '')\n",
    "    for word in filter_words:\n",
    "        regex_word = r\"\\b\" + word + r\"\\b\"\n",
    "        context = context.str.replace(regex_word, '')\n",
    "    return context\n",
    "    \n",
    "# filter_words = ['the','fda','us','usfda','hk','doh','ema','tga','taiwan','dh','es','pdr','canada','hsa','china','malaysia','singapore','germany','mg',\n",
    "#                'ml','product','products','nbsp', 'warning letter','mah','company','please','uk']\n",
    "filter_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56f820f-8d00-47d4-aed1-9727608a31a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overseas.loc[:, \"text\"] = text_clean(overseas[\"text\"])\n",
    "overseas.loc[:, \"text\"] = stop_words(overseas[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e20a9db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique terms: 16859\n"
     ]
    }
   ],
   "source": [
    "# word count\n",
    "from itertools import chain\n",
    "unique_terms = overseas.text.str.split()\n",
    "unique_terms = set(list(chain(*unique_terms)))\n",
    "print('Count of unique terms:',len(unique_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0f36f",
   "metadata": {},
   "source": [
    "### Load BioBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2503eef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmondteoch\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\desmondteoch\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT (discharge summaries)\n",
    "path = r\"C:\\Users\\desmondteoch\\Documents\\CVU\\Capstone\\biobert_pretrain_output_disch_100000\"\n",
    "tokenizer = BertTokenizer.from_pretrained(path, local_files_only=True)\n",
    "bert = TFAutoModel.from_pretrained(path, from_pt=True)\n",
    "vocab_file = '/biobert_pretrain_output_disch_100000/vocab.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265680b",
   "metadata": {},
   "source": [
    "### Split data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f55bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 'text' and 'label' columns\n",
    "X = overseas['text']\n",
    "y = overseas['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa141d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (571,)\n",
      "Validation data: (143,)\n",
      "Testing data: (179,)\n"
     ]
    }
   ],
   "source": [
    "# split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=24)\n",
    "\n",
    "# further split training into training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=24)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}\\nValidation data: {X_val.shape}\\nTesting data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69e81d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_lower_case = True # use model for lowercase\n",
    "max_len = 512\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=max_len):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64c0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training and test sets into keras readable format\n",
    "inputs_train = bert_encode(X_train, tokenizer, max_len=max_len)\n",
    "inputs_val = bert_encode(X_val, tokenizer, max_len=max_len)\n",
    "inputs_test = bert_encode(X_test, tokenizer, max_len=max_len)\n",
    "labels_train = y_train.values.astype(int)\n",
    "labels_val = y_val.values.astype(int)\n",
    "labels_test = y_test.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34ff8aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "\n",
    "def build_model(bert_layer, max_len=max_len):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    b = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    net = b.last_hidden_state\n",
    "    net = tf.keras.layers.SpatialDropout1D(0.2)(net)\n",
    "    net = tf.keras.layers.GlobalMaxPooling1D()(net)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(net)  # 1 output node and sigmoid activation for binary classification\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-5,\n",
    "                                                                decay_steps=10000,\n",
    "                                                                decay_rate=0.9)\n",
    "    \n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss='binary_crossentropy',  # for binary classification\n",
    "                  metrics=['accuracy',\n",
    "                           tf.keras.metrics.Precision(),\n",
    "                           tf.keras.metrics.Recall(),\n",
    "                           tf.keras.metrics.AUC()]\n",
    "                 )\n",
    "    \n",
    "    # Early stopping if no further improvements during training\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                      patience=3,\n",
    "                                                      restore_best_weights=True)\n",
    "    \n",
    "    model._name = 'BioBERT_Binary_Model'\n",
    "    \n",
    "    return model, early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4950c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "72/72 [==============================] - 1176s 16s/step - loss: 0.7279 - accuracy: 0.5902 - precision_1: 0.6453 - recall_1: 0.7443 - auc_1: 0.5453 - val_loss: 0.6308 - val_accuracy: 0.6434 - val_precision_1: 0.6454 - val_recall_1: 0.9891 - val_auc_1: 0.6530\n",
      "Epoch 2/3\n",
      "72/72 [==============================] - 1158s 16s/step - loss: 0.6393 - accuracy: 0.6252 - precision_1: 0.6590 - recall_1: 0.8125 - auc_1: 0.6488 - val_loss: 0.6076 - val_accuracy: 0.6643 - val_precision_1: 0.6719 - val_recall_1: 0.9348 - val_auc_1: 0.7002\n",
      "Epoch 3/3\n",
      "72/72 [==============================] - 1157s 16s/step - loss: 0.5871 - accuracy: 0.6935 - precision_1: 0.7133 - recall_1: 0.8409 - auc_1: 0.7219 - val_loss: 0.5939 - val_accuracy: 0.6643 - val_precision_1: 0.6964 - val_recall_1: 0.8478 - val_auc_1: 0.7367\n",
      "5/5 [==============================] - 74s 14s/step - loss: 0.5939 - accuracy: 0.6643 - precision_1: 0.6964 - recall_1: 0.8478 - auc_1: 0.7367\n",
      "Metrics for epoch=3, batch_size=8: [0.5939305424690247, 0.6643356680870056, 0.6964285969734192, 0.8478260636329651, 0.7366794347763062].\n",
      "INFO:tensorflow:Assets written to: ./models/BioBERT_epoch_3_batchsize_8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/BioBERT_epoch_3_batchsize_8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 2004s 55s/step - loss: 0.6961 - accuracy: 0.5832 - precision_2: 0.6301 - recall_2: 0.7841 - auc_2: 0.5604 - val_loss: 0.6257 - val_accuracy: 0.6853 - val_precision_2: 0.6767 - val_recall_2: 0.9783 - val_auc_2: 0.6566\n",
      "Epoch 2/3\n",
      "36/36 [==============================] - 2025s 56s/step - loss: 0.6385 - accuracy: 0.6322 - precision_2: 0.6644 - recall_2: 0.8153 - auc_2: 0.6478 - val_loss: 0.5948 - val_accuracy: 0.6713 - val_precision_2: 0.6744 - val_recall_2: 0.9457 - val_auc_2: 0.7052\n",
      "Epoch 3/3\n",
      "36/36 [==============================] - 2053s 57s/step - loss: 0.6044 - accuracy: 0.6550 - precision_2: 0.6749 - recall_2: 0.8494 - auc_2: 0.7051 - val_loss: 0.5726 - val_accuracy: 0.6853 - val_precision_2: 0.6942 - val_recall_2: 0.9130 - val_auc_2: 0.7290\n",
      "5/5 [==============================] - 74s 14s/step - loss: 0.5726 - accuracy: 0.6853 - precision_2: 0.6942 - recall_2: 0.9130 - auc_2: 0.7290\n",
      "Metrics for epoch=3, batch_size=16: [0.5725988745689392, 0.6853147149085999, 0.6942148804664612, 0.9130434989929199, 0.729006826877594].\n",
      "INFO:tensorflow:Assets written to: ./models/BioBERT_epoch_3_batchsize_16\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/BioBERT_epoch_3_batchsize_16\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 1202s 16s/step - loss: 0.7024 - accuracy: 0.5779 - precision_3: 0.6270 - recall_3: 0.7784 - auc_3: 0.5528 - val_loss: 0.5890 - val_accuracy: 0.6783 - val_precision_3: 0.6716 - val_recall_3: 0.9783 - val_auc_3: 0.7255\n",
      "Epoch 2/5\n",
      "72/72 [==============================] - 1170s 16s/step - loss: 0.6115 - accuracy: 0.6602 - precision_3: 0.6985 - recall_3: 0.7898 - auc_3: 0.6868 - val_loss: 0.5684 - val_accuracy: 0.6783 - val_precision_3: 0.7500 - val_recall_3: 0.7500 - val_auc_3: 0.7436\n",
      "Epoch 3/5\n",
      "72/72 [==============================] - 2601s 36s/step - loss: 0.4943 - accuracy: 0.7671 - precision_3: 0.7874 - recall_3: 0.8523 - auc_3: 0.8252 - val_loss: 0.6071 - val_accuracy: 0.6923 - val_precision_3: 0.6875 - val_recall_3: 0.9565 - val_auc_3: 0.7757\n",
      "Epoch 4/5\n",
      "72/72 [==============================] - 2632s 37s/step - loss: 0.4440 - accuracy: 0.7793 - precision_3: 0.8122 - recall_3: 0.8352 - auc_3: 0.8664 - val_loss: 0.5456 - val_accuracy: 0.6853 - val_precision_3: 0.7009 - val_recall_3: 0.8913 - val_auc_3: 0.8004\n",
      "Epoch 5/5\n",
      "72/72 [==============================] - 2590s 36s/step - loss: 0.3811 - accuracy: 0.8301 - precision_3: 0.8612 - recall_3: 0.8636 - auc_3: 0.9067 - val_loss: 0.5169 - val_accuracy: 0.7413 - val_precision_3: 0.8022 - val_recall_3: 0.7935 - val_auc_3: 0.8026\n",
      "5/5 [==============================] - 131s 24s/step - loss: 0.5169 - accuracy: 0.7413 - precision_3: 0.8022 - recall_3: 0.7935 - auc_3: 0.8026\n",
      "Metrics for epoch=5, batch_size=8: [0.5169349312782288, 0.7412587404251099, 0.8021978139877319, 0.79347825050354, 0.802642822265625].\n",
      "INFO:tensorflow:Assets written to: ./models/BioBERT_epoch_5_batchsize_8\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/BioBERT_epoch_5_batchsize_8\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 2087s 57s/step - loss: 0.4810 - accuracy: 0.7653 - precision_4: 0.7946 - recall_4: 0.8352 - auc_4: 0.8391 - val_loss: 0.5427 - val_accuracy: 0.7133 - val_precision_4: 0.8072 - val_recall_4: 0.7283 - val_auc_4: 0.7991\n",
      "Epoch 2/5\n",
      "36/36 [==============================] - 2044s 57s/step - loss: 0.3473 - accuracy: 0.8511 - precision_4: 0.8825 - recall_4: 0.8750 - auc_4: 0.9212 - val_loss: 0.6177 - val_accuracy: 0.7203 - val_precision_4: 0.7241 - val_recall_4: 0.9130 - val_auc_4: 0.7931\n",
      "Epoch 3/5\n",
      "36/36 [==============================] - 2090s 58s/step - loss: 0.2372 - accuracy: 0.9037 - precision_4: 0.9160 - recall_4: 0.9290 - auc_4: 0.9649 - val_loss: 0.6223 - val_accuracy: 0.6923 - val_precision_4: 0.7927 - val_recall_4: 0.7065 - val_auc_4: 0.8047\n",
      "Epoch 4/5\n",
      "36/36 [==============================] - 3563s 100s/step - loss: 0.1426 - accuracy: 0.9545 - precision_4: 0.9631 - recall_4: 0.9631 - auc_4: 0.9884 - val_loss: 0.7433 - val_accuracy: 0.7343 - val_precision_4: 0.7647 - val_recall_4: 0.8478 - val_auc_4: 0.7892\n",
      "5/5 [==============================] - 78s 15s/step - loss: 0.5427 - accuracy: 0.7133 - precision_4: 0.8072 - recall_4: 0.7283 - auc_4: 0.7991\n",
      "Metrics for epoch=5, batch_size=16: [0.5426627993583679, 0.7132866978645325, 0.8072289228439331, 0.72826087474823, 0.799126148223877].\n",
      "INFO:tensorflow:Assets written to: ./models/BioBERT_epoch_5_batchsize_16\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/BioBERT_epoch_5_batchsize_16\\assets\n"
     ]
    }
   ],
   "source": [
    "## un-hash lines below to train the model\n",
    "# Warning: takes a long time to run. load final model below, unless re-training is required.\n",
    "# hyperparameter tuning\n",
    "num_epochs = [3, 5]\n",
    "batch_sizes = [8, 16]\n",
    "\n",
    "# store metrics for each set of hyperparameters\n",
    "metrics = {}\n",
    "\n",
    "for epoch in num_epochs:\n",
    "    for batch_size in batch_sizes:\n",
    "        # build model\n",
    "        model, early_stopping = build_model(bert, max_len=max_len)\n",
    "        \n",
    "        # train model\n",
    "        train_history = model.fit(\n",
    "            inputs_train, \n",
    "            labels_train,\n",
    "            validation_data=(inputs_val, labels_val),\n",
    "            epochs=epoch,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # evaluate model on validation set\n",
    "        evaluation = model.evaluate(inputs_val, labels_val)\n",
    "        \n",
    "        # add metrics to dict\n",
    "        metrics[(epoch, batch_size)] = evaluation\n",
    "        \n",
    "        # print progress\n",
    "        print(f\"Metrics for epoch={epoch}, batch_size={batch_size}: {evaluation}.\")\n",
    "        \n",
    "        ## save parameters\n",
    "        model.save(f\"./models/BioBERT_epoch_{epoch}_batchsize_{batch_size}\")\n",
    "        \n",
    "        ## save model weights\n",
    "        weights_path = f\"./models/BioBERT_epoch_{epoch}_batchsize_{batch_size}_weights.h5\"\n",
    "        model.save_weights(weights_path)\n",
    "        \n",
    "        # plot loss vs epoch curve\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_history.history['loss'])\n",
    "        plt.plot(train_history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xticks(range(0,epoch))\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "        plt.savefig(f'./images/loss_curve_epoch_{epoch}_batchsize_{batch_size}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daf90544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: (3, 8), Metrics: [0.5939305424690247, 0.6643356680870056, 0.6964285969734192, 0.8478260636329651, 0.7366794347763062]\n"
     ]
    }
   ],
   "source": [
    "# print best hyperparameters i.e. lowest validation loss\n",
    "best_hyperparameters = min(metrics, key=lambda x: metrics[x][1])\n",
    "print(f\"Best Hyperparameters: {best_hyperparameters}, Metrics: {metrics[best_hyperparameters]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5323326f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(3, 8): [0.5939305424690247,\n",
       "  0.6643356680870056,\n",
       "  0.6964285969734192,\n",
       "  0.8478260636329651,\n",
       "  0.7366794347763062],\n",
       " (3, 16): [0.5725988745689392,\n",
       "  0.6853147149085999,\n",
       "  0.6942148804664612,\n",
       "  0.9130434989929199,\n",
       "  0.729006826877594],\n",
       " (5, 8): [0.5169349312782288,\n",
       "  0.7412587404251099,\n",
       "  0.8021978139877319,\n",
       "  0.79347825050354,\n",
       "  0.802642822265625],\n",
       " (5, 16): [0.5426627993583679,\n",
       "  0.7132866978645325,\n",
       "  0.8072289228439331,\n",
       "  0.72826087474823,\n",
       "  0.799126148223877]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bb3e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load specific model for testing\n",
    "epoch = 3\n",
    "batch_size = 8\n",
    "\n",
    "# model = keras.models.load_model(f'./models/BioBERT_BiLSTM_epoch_{epoch}_batchsize_{batch_size}')\n",
    "model.load_weights(f'./models/BioBERT_epoch_{epoch}_batchsize_{batch_size}_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c61db11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 89s 14s/step\n"
     ]
    }
   ],
   "source": [
    "# predict on unseen data - testing set\n",
    "model_pred = model.predict(inputs_test)\n",
    "threshold = 0.5\n",
    "# pred_test = test_set['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4999f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.41      0.45        54\n",
      "           1       0.76      0.82      0.79       125\n",
      "\n",
      "    accuracy                           0.70       179\n",
      "   macro avg       0.63      0.62      0.62       179\n",
      "weighted avg       0.68      0.70      0.69       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "model_pred_binary = (model_pred >= threshold).astype(int)\n",
    "print(classification_report(labels_test, model_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1acd1a75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22  32]\n",
      " [ 22 103]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(labels_test, model_pred_binary))\n",
    "#[[TN  FP]\n",
    "# [FN  TP]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a47b43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 194s 32s/step\n",
      "AUC: 0.6859259259259259\n"
     ]
    }
   ],
   "source": [
    "# Get AUC score\n",
    "model_pred_proba = model.predict(inputs_test)\n",
    "auc = roc_auc_score(labels_test, model_pred_proba)\n",
    "\n",
    "print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e531cd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
